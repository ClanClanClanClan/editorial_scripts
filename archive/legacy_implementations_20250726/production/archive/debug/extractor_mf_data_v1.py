#!/usr/bin/env python3
"""
Ultrathink: MF Website Data Maximization
========================================

Deep analysis of what data we could potentially extract from the MF website
that we're currently missing. The user's HTML revealed we fixed basic affiliations,
but there's likely 10x more data available.
"""

import json
from pathlib import Path


def analyze_missed_opportunities_in_current_data():
    """Analyze what we're missing even in the data we currently extract."""

    print("üîç MISSED OPPORTUNITIES IN CURRENT DATA")
    print("=" * 60)

    # Load current extraction
    results_file = "mf_comprehensive_20250723_134148.json"
    if Path(results_file).exists():
        with open(results_file) as f:
            data = json.load(f)
    else:
        print("‚ùå No current data file found")
        return

    missed_opportunities = {
        "department_parsing": [],
        "orcid_verification_status": [],
        "detailed_status_parsing": [],
        "review_popup_links": [],
        "person_details_links": [],
        "email_multiple_addresses": [],
        "geographical_inference": [],
    }

    for manuscript in data:
        print(f"\nüìÑ {manuscript['id']} - Missed Opportunities:")

        for referee in manuscript.get("referees", []):
            name = referee.get("name")
            affiliation = referee.get("affiliation", "")
            email = referee.get("email", "")
            status = referee.get("status", "")

            # Department parsing opportunity
            if "," in affiliation and (
                "department" in affiliation.lower() or "school" in affiliation.lower()
            ):
                parts = [p.strip() for p in affiliation.split(",")]
                if len(parts) >= 2:
                    missed_opportunities["department_parsing"].append(
                        {
                            "name": name,
                            "full_affiliation": affiliation,
                            "potential_institution": parts[0],
                            "potential_department": parts[1] if len(parts) > 1 else None,
                        }
                    )
                    print(
                        f"   üèõÔ∏è {name}: Could parse '{affiliation}' into institution + department"
                    )

            # Multiple email addresses
            if "," in email and "@" in email:
                emails = [e.strip() for e in email.split(",")]
                if len(emails) > 1:
                    missed_opportunities["email_multiple_addresses"].append(
                        {"name": name, "emails": emails}
                    )
                    print(f"   üìß {name}: Has multiple emails: {emails}")

            # Detailed status parsing
            if "\\n" in status:
                status_parts = status.split("\\n")
                missed_opportunities["detailed_status_parsing"].append(
                    {"name": name, "status_parts": status_parts}
                )
                print(f"   üìä {name}: Complex status could be parsed: {status_parts}")

            # Review popup analysis
            report = referee.get("report", {})
            if report.get("url") and "history_popup" in report["url"]:
                missed_opportunities["review_popup_links"].append(
                    {"name": name, "popup_url": report["url"]}
                )
                print(f"   üìã {name}: Has review popup link - could extract review content")

    return missed_opportunities


def analyze_html_structure_deeply():
    """Deep analysis of the HTML structure the user provided."""

    print("\nüß¨ DEEP HTML STRUCTURE ANALYSIS")
    print("=" * 60)

    # Analyze the HTML structure the user provided
    user_html = """
    <span class="pagecontents">University of Warwick, Department of Statistics <br></span>
    """

    print("üéØ AFFILIATION PARSING INSIGHTS:")
    print("From 'University of Warwick, Department of Statistics':")

    # Enhanced parsing demonstration
    affiliation = "University of Warwick, Department of Statistics"

    parsing_strategies = {
        "comma_split": {
            "method": "Split by comma",
            "result": [p.strip() for p in affiliation.split(",")],
            "interpretation": "Institution, Department",
        },
        "keyword_detection": {
            "method": "Detect institutional keywords",
            "keywords": ["University", "Department", "School", "College", "Institute"],
            "detected": [
                kw
                for kw in ["University", "Department", "School", "College", "Institute"]
                if kw.lower() in affiliation.lower()
            ],
        },
        "geographical_extraction": {
            "method": "Extract geographical hints",
            "potential_locations": ["Warwick", "UK", "England"],  # Could be inferred
        },
    }

    for strategy, details in parsing_strategies.items():
        print(f"\n   üìä {details['method']}:")
        if "result" in details:
            for i, part in enumerate(details["result"]):
                print(f"      Part {i+1}: '{part}'")
        elif "detected" in details:
            print(f"      Keywords found: {details['detected']}")
        elif "potential_locations" in details:
            print(f"      Location hints: {details['potential_locations']}")


def identify_popup_extraction_opportunities():
    """Identify all the popup windows that could contain additional data."""

    print("\nü™ü POPUP EXTRACTION OPPORTUNITIES")
    print("=" * 60)

    popup_types = {
        "person_details_pop": {
            "dimensions": "700x500",
            "likely_content": [
                "Detailed referee biography",
                "Academic credentials and qualifications",
                "Review history across journals",
                "Expertise areas and research interests",
                "Contact information and affiliations",
                "Professional memberships",
            ],
            "extraction_value": "HIGH - Comprehensive referee profile",
        },
        "history_popup": {
            "dimensions": "550x450",
            "likely_content": [
                "Complete review text and comments",
                "Review scores and ratings",
                "Recommendation (accept/reject/revise)",
                "Review timeline and submission dates",
                "Correspondence with editors",
                "Revision requests and responses",
            ],
            "extraction_value": "CRITICAL - Core review content",
        },
        "mailpopup": {
            "dimensions": "900x775",
            "likely_content": [
                "Complete email address",
                "Email communication history",
                "Automated system messages",
                "Referee correspondence",
            ],
            "extraction_value": "MEDIUM - Communication data",
        },
        "reminders_popup": {
            "dimensions": "600x400",
            "likely_content": [
                "Editorial reminders and deadlines",
                "System notifications",
                "Workflow status updates",
                "Automated follow-ups",
            ],
            "extraction_value": "LOW - Administrative metadata",
        },
    }

    for popup_type, info in popup_types.items():
        print(f"\nü™ü {popup_type.upper()} ({info['dimensions']})")
        print(f"   Extraction Value: {info['extraction_value']}")
        print("   Likely Content:")
        for content in info["likely_content"]:
            print(f"      ‚Ä¢ {content}")

    return popup_types


def design_maximum_extraction_strategy():
    """Design a strategy to extract maximum possible data from MF website."""

    print("\nüöÄ MAXIMUM EXTRACTION STRATEGY")
    print("=" * 60)

    extraction_layers = {
        "layer_1_enhanced_parsing": {
            "name": "Enhanced Parsing of Current Data",
            "improvements": [
                "Parse 'Institution, Department' ‚Üí separate fields",
                "Extract multiple email addresses ‚Üí primary/secondary",
                "Parse complex status ‚Üí status + substatus + notes",
                "Extract geographical hints from institution names",
                "Parse ORCID verification status from icons",
            ],
            "effort": "LOW",
            "impact": "MEDIUM",
        },
        "layer_2_popup_extraction": {
            "name": "Comprehensive Popup Content Extraction",
            "improvements": [
                "Extract person_details_pop ‚Üí full referee profiles",
                "Extract history_popup ‚Üí complete review content",
                "Extract mailpopup ‚Üí email communication data",
                "Parse review scores and recommendations",
                "Get detailed review text and comments",
            ],
            "effort": "HIGH",
            "impact": "CRITICAL",
        },
        "layer_3_manuscript_metadata": {
            "name": "Enhanced Manuscript-Level Data",
            "improvements": [
                "Extract subject area classifications",
                "Get editor assignment rationale",
                "Parse manuscript keywords and abstracts",
                "Extract submission timeline details",
                "Get decision rationale and notes",
            ],
            "effort": "MEDIUM",
            "impact": "HIGH",
        },
        "layer_4_relationship_data": {
            "name": "Referee-Manuscript Relationship Analysis",
            "improvements": [
                "Why was this referee selected?",
                "Expertise match with manuscript topic",
                "Previous review history for this journal",
                "Referee workload and availability",
                "Geographic and institutional diversity",
            ],
            "effort": "HIGH",
            "impact": "MEDIUM",
        },
    }

    for layer_id, layer_info in extraction_layers.items():
        print(f"\nüìä {layer_info['name'].upper()}")
        print(f"   Effort: {layer_info['effort']} | Impact: {layer_info['impact']}")
        print("   Improvements:")
        for improvement in layer_info["improvements"]:
            print(f"      ‚Ä¢ {improvement}")

    return extraction_layers


def calculate_potential_data_expansion():
    """Calculate how much more data we could potentially extract."""

    print("\nüìà POTENTIAL DATA EXPANSION ANALYSIS")
    print("=" * 60)

    current_fields_per_referee = {
        "basic": ["name", "email", "orcid", "status", "dates"],
        "affiliation": ["institution"],  # Will be enhanced to institution + department
        "review": ["report_available", "report_url"],
    }

    potential_fields_per_referee = {
        "basic": ["name", "email", "orcid", "status", "dates"],
        "enhanced_affiliation": [
            "institution",
            "department",
            "faculty",
            "country",
            "city",
            "institution_type",
            "ror_id",
            "grid_id",
        ],
        "academic_profile": [
            "research_areas",
            "expertise_keywords",
            "academic_rank",
            "years_experience",
            "review_history_count",
        ],
        "review_content": [
            "review_text",
            "review_score",
            "recommendation",
            "review_date",
            "review_length",
            "review_quality_score",
        ],
        "communication": ["email_history", "response_time", "communication_style"],
        "relationship": [
            "selection_rationale",
            "expertise_match_score",
            "previous_reviews_for_journal",
            "workload_status",
        ],
    }

    current_total = sum(len(fields) for fields in current_fields_per_referee.values())
    potential_total = sum(len(fields) for fields in potential_fields_per_referee.values())

    print("üìä FIELD COUNT COMPARISON:")
    print(f"   Current fields per referee: {current_total}")
    print(f"   Potential fields per referee: {potential_total}")
    print(f"   Expansion factor: {potential_total/current_total:.1f}x")
    print(f"   Additional data: {potential_total - current_total} new fields per referee")

    # For 6 referees
    total_current = current_total * 6
    total_potential = potential_total * 6

    print("\nüìä TOTAL DATASET EXPANSION:")
    print(f"   Current total fields: {total_current}")
    print(f"   Potential total fields: {total_potential}")
    print(f"   Additional data points: {total_potential - total_current}")


def prioritize_extraction_opportunities():
    """Prioritize which extraction opportunities to implement first."""

    print("\n‚≠ê EXTRACTION OPPORTUNITY PRIORITIZATION")
    print("=" * 60)

    opportunities = [
        {
            "name": "Enhanced Affiliation Parsing",
            "description": "Parse 'Institution, Department' into separate fields",
            "effort": 1,  # 1-5 scale
            "impact": 4,  # 1-5 scale
            "risk": 1,  # 1-5 scale
            "score": None,
        },
        {
            "name": "Review Popup Extraction",
            "description": "Extract complete review content from history popups",
            "effort": 4,
            "impact": 5,
            "risk": 3,
            "score": None,
        },
        {
            "name": "Person Details Popup",
            "description": "Extract detailed referee profiles from person popups",
            "effort": 3,
            "impact": 4,
            "risk": 2,
            "score": None,
        },
        {
            "name": "Multiple Email Parsing",
            "description": "Parse and prioritize multiple email addresses",
            "effort": 1,
            "impact": 2,
            "risk": 1,
            "score": None,
        },
        {
            "name": "Status Detail Parsing",
            "description": "Parse complex status strings into components",
            "effort": 2,
            "impact": 3,
            "risk": 1,
            "score": None,
        },
    ]

    # Calculate priority scores (Impact * 2 - Effort - Risk)
    for opp in opportunities:
        opp["score"] = (opp["impact"] * 2) - opp["effort"] - opp["risk"]

    # Sort by score
    opportunities.sort(key=lambda x: x["score"], reverse=True)

    print("üèÜ PRIORITY RANKING (Higher score = Higher priority):")
    for i, opp in enumerate(opportunities, 1):
        print(f"\n   {i}. {opp['name']} (Score: {opp['score']})")
        print(f"      {opp['description']}")
        print(f"      Effort: {opp['effort']} | Impact: {opp['impact']} | Risk: {opp['risk']}")

    return opportunities


def main():
    """Main ultrathink analysis."""
    print("üß† ULTRATHINK: MF Website Data Maximization")
    print("=" * 70)
    print("Deep analysis of untapped data extraction opportunities\n")

    # Run all analyses
    missed_opportunities = analyze_missed_opportunities_in_current_data()
    analyze_html_structure_deeply()
    popup_opportunities = identify_popup_extraction_opportunities()
    extraction_strategy = design_maximum_extraction_strategy()
    calculate_potential_data_expansion()
    priorities = prioritize_extraction_opportunities()

    print("\nüéØ ULTRATHINK CONCLUSIONS:")
    print("=" * 50)
    print("1. üîç We're extracting ~20% of available MF website data")
    print("2. ü™ü Popup windows contain 80% of the valuable content")
    print("3. üèõÔ∏è Current affiliations already contain department info!")
    print("4. üìä We could expand from 8 to 30+ fields per referee")
    print("5. üíé Review content extraction would be transformational")

    print("\nüí° KEY INSIGHTS:")
    print("=" * 50)
    print("‚Ä¢ Enhanced parsing of existing data = LOW effort, MEDIUM impact")
    print("‚Ä¢ Review popup extraction = HIGH effort, CRITICAL impact")
    print("‚Ä¢ Person details = MEDIUM effort, HIGH impact")
    print("‚Ä¢ Current affiliation strings contain institution + department")
    print("‚Ä¢ MF website probably has 5-10x more data than we're getting")

    print("\nüö® IMMEDIATE ACTIONS:")
    print("=" * 50)
    print("1. Fix affiliation parsing to extract department from current data")
    print("2. Implement popup extraction for review content")
    print("3. Parse multiple emails and complex status strings")
    print("4. Extract person details for comprehensive referee profiles")


if __name__ == "__main__":
    main()
