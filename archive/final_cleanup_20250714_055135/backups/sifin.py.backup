from core.base import JournalBase
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from bs4 import BeautifulSoup
import re
import logging
import time
from typing import List, Dict, Tuple

def dismiss_orcid_cookie_modal(driver, timeout=8, debug=True):
    """
    Dismisses the ORCID cookie modal if present.
    Tries both 'Autoriser tous les cookies', 'Tout refuser', and 'Accepter'.
    """
    btn_xpaths = [
        "//button[contains(translate(., 'AUTORISER', 'autoriser'), 'autoriser tous les cookies')]",
        "//button[contains(translate(., 'TOUT REFUSER', 'tout refuser'), 'tout refuser')]",
        "//button[contains(translate(., 'ACCEPTER', 'accepter'), 'accepter')]",
        "//button[contains(.,'Accept all cookies')]",
        "//button[contains(.,'Refuse all')]"
    ]
    try:
        for _ in range(timeout):
            for xp in btn_xpaths:
                try:
                    btn = driver.find_element(By.XPATH, xp)
                    if btn.is_displayed() and btn.is_enabled():
                        btn.click()
                        if debug:
                            print("[SIFIN] ORCID cookie modal dismissed.")
                        time.sleep(1)
                        return
                except Exception:
                    pass
            time.sleep(1)
    except Exception:
        pass
    if debug:
        print("[SIFIN] ORCID cookie modal not found or already dismissed.")

class SIFINJournal(JournalBase):
    SIFIN_URL = "https://sifin.siam.org/cgi-bin/main.plex"
    SITE_PREFIX = "https://sifin.siam.org/"

    def __init__(self, driver):
        super().__init__(driver)

    def get_url(self) -> str:
        return self.SIFIN_URL

    def remove_cookie_banner(self):
        js_hide = """
        for(let sel of [
            '#cookie-policy-layer-bg',
            '.cc_banner-wrapper',
            '#cookie-policy-layer',
            '#onetrust-banner-sdk',
            '.onetrust-pc-dark-filter'
        ]) {
            let el = document.querySelector(sel);
            if(el) el.style.display='none';
        }
        """
        try:
            self.driver.execute_script(js_hide)
        except Exception:
            pass

    def batch_fetch_emails(self, email_urls: List[str]) -> Dict[str, str]:
        driver = self.driver
        email_cache = {}
        unique_urls = set(u for u in email_urls if u)
        for url in unique_urls:
            full_url = url if url.startswith("http") else self.SITE_PREFIX + url.lstrip("/")
            try:
                driver.get(full_url)
                # Accept ORCID cookie modal if present
                dismiss_orcid_cookie_modal(driver, timeout=8, debug=True)
                WebDriverWait(driver, 5).until(lambda d: "@" in d.page_source)
            except Exception:
                pass
            soup = BeautifulSoup(driver.page_source, "html.parser")
            email_tag = soup.find("a", href=re.compile(r"mailto:"))
            if email_tag:
                email_cache[url] = email_tag.text.strip()
                continue
            found = False
            for td in soup.find_all("td"):
                txt = td.get_text(strip=True)
                if "@" in txt and "." in txt:
                    email_cache[url] = txt
                    found = True
                    break
            if not found:
                email_cache[url] = ""
        return email_cache

    def extract_sifin_table(self) -> Tuple[Dict, List[str]]:
        soup = BeautifulSoup(self.driver.page_source, "html.parser")
        table = soup.find("table", id="ms_details_expanded")
        if not table:
            return None, []
        info = {}
        referees = []
        email_urls = []
        for row in table.find_all("tr"):
            th = row.find("th")
            td = row.find("td")
            if not th or not td:
                continue
            label = th.get_text(strip=True)
            val = td.decode_contents()
            if label.startswith("Manuscript #"):
                info["Manuscript #"] = td.get_text(strip=True)
            elif label == "Title":
                info["Title"] = td.get_text(strip=True)
            elif label == "Submission Date":
                info["Submitted"] = td.get_text(strip=True)
            elif label == "Current Stage":
                info["Current Stage"] = td.get_text(strip=True)
            elif label == "Referees":
                if "N/A" in val:
                    continue
                for ref_a in td.find_all("a"):
                    ref_name = ref_a.get_text(strip=True)
                    ref_url = ref_a.get("href")
                    due = ""
                    next_font = ref_a.find_next("font")
                    if next_font and "Due:" in next_font.text:
                        match = re.search(r"Due:\s*([\d\-]+)", next_font.text)
                        if match:
                            due = match.group(1)
                    abs_url = ref_url if ref_url.startswith("http") else self.SITE_PREFIX + ref_url.lstrip("/")
                    email_urls.append(abs_url)
                    referees.append({
                        "Referee Name": ref_name,
                        "Referee URL": abs_url,
                        "Status": "Accepted",
                        "Due Date": due
                    })
            elif "Potential Referees" in label:
                soup_inner = BeautifulSoup(str(td), "html.parser")
                for a in soup_inner.find_all("a"):
                    ref_name = a.get_text(strip=True)
                    ref_url = a.get("href", "")
                    abs_url = ref_url if ref_url.startswith("http") else self.SITE_PREFIX + ref_url.lstrip("/")
                    sibs = []
                    node = a.next_sibling
                    count = 0
                    while node and count < 8:
                        if hasattr(node, "name") and node.name == "a":
                            break
                        if isinstance(node, str):
                            sibs.append(node.strip())
                        node = node.next_sibling
                        count += 1
                    after = " ".join(sibs)
                    m = re.search(r"Status:\s*([^)]+)", after)
                    status = m.group(1) if m else ""
                    if status.lower().strip() == "declined":
                        continue
                    if status.lower().strip() == "contacted":
                        email_urls.append(abs_url)
                        referees.append({
                            "Referee Name": ref_name,
                            "Referee URL": abs_url,
                            "Status": "Contacted",
                            "Due Date": ""
                        })
        return {
            "Manuscript #": info.get("Manuscript #", ""),
            "Title": info.get("Title", ""),
            "Submitted": info.get("Submitted", ""),
            "Current Stage": info.get("Current Stage", ""),
            "Referees": referees,
        }, email_urls

    def scrape_manuscripts_and_emails(self) -> List[Dict]:
        driver = self.driver
        try:
            logging.info("Navigating to SIFIN dashboard...")
            driver.get(self.get_url())
            self.remove_cookie_banner()
            self.login()
            self.remove_cookie_banner()
            time.sleep(1)
            soup = BeautifulSoup(driver.page_source, "html.parser")
            ms_links = []
            for assoc_ed_section in soup.find_all("tbody", {"role": "assoc_ed"}):
                for row in assoc_ed_section.find_all("tr", class_="ndt_task"):
                    link = row.find("a", class_="ndt_task_link")
                    if not link:
                        continue
                    if not link.text.strip().startswith("#"):
                        continue
                    href = link.get("href", "")
                    if "form_type=view_ms" in href:
                        full_url = href if href.startswith("http") else self.SITE_PREFIX + href.lstrip("/")
                        ms_links.append(full_url)
            if not ms_links:
                logging.warning("No assigned manuscripts found in SIFIN dashboard.")
                return []
            results = []
            email_urls = []
            for url in ms_links:
                try:
                    driver.get(url)
                    time.sleep(1.0)
                    ms_data, ms_email_urls = self.extract_sifin_table()
                    if ms_data:
                        results.append(ms_data)
                        email_urls.extend(ms_email_urls)
                except Exception as e:
                    logging.error(f"Error scraping {url}: {e}")
                    continue
            email_cache = self.batch_fetch_emails(email_urls)
            for ms in results:
                for ref in ms["Referees"]:
                    ref['Referee Email'] = email_cache.get(ref["Referee URL"], "")
            return results
        finally:
            pass  # Driver cleanup handled by the orchestrator