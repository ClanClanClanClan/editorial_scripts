from core.base import JournalBase
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
from bs4 import BeautifulSoup
import os
import time
import logging
import re
from datetime import datetime

from core.email_utils import (
    fetch_latest_verification_code,
    fetch_starred_emails,
    robust_match_email_for_referee_mf,
)

def normalize_name(name):
    """Convert 'Last, First' to 'First Last', clean up."""
    name = name.replace('(contact)', '').replace(';', '').strip()
    if ',' in name:
        last, first = [part.strip() for part in name.split(',', 1)]
        return f"{first} {last}"
    return name.strip()

def robust_parse_date(date_str):
    """Try to parse dates like 26-Mar-2025 and return datetime.date, else None."""
    if not date_str or not date_str.strip():
        return None
    for fmt in ("%d-%b-%Y", "%d/%m/%Y", "%Y-%m-%d"):
        try:
            return datetime.strptime(date_str.strip(), fmt).date()
        except Exception:
            continue
    return None

def mf_status_normalize(status_raw):
    s = (status_raw or "").strip().lower()
    if s in {"agreed", "accepted", "overdue"}:
        return "Accepted"
    return "Contacted"

class MFJournal(JournalBase):
    MF_URL = "https://mc.manuscriptcentral.com/mafi"

    def __init__(self, driver, debug=True, chrome_profile_dir=None):
        super().__init__(driver)
        self.debug = debug
        self.chrome_profile_dir = chrome_profile_dir
        self.logger = logging.getLogger("[MF]")
        self.logger.setLevel(logging.DEBUG if debug else logging.INFO)
        self.activation_required = False

    def get_url(self):
        return self.MF_URL

    def wait_if_human_check(self, max_time=30):
        try:
            WebDriverWait(self.driver, 2).until(
                EC.presence_of_element_located((By.XPATH, "//iframe[contains(@title, 'reCAPTCHA')]"))
            )
            iframe = self.driver.find_element(By.XPATH, "//iframe[contains(@title, 'reCAPTCHA')]")
            self.driver.switch_to.frame(iframe)
            checkbox = self.driver.find_element(By.ID, "recaptcha-anchor")
            checkbox.click()
            self.driver.switch_to.default_content()
            self.logger.info("Clicked reCAPTCHA checkbox.")
            time.sleep(2)
        except Exception:
            self.logger.debug("No reCAPTCHA present.")

    def login(self):
        self.login_standard()

    def login_standard(self):
        driver = self.driver
        self.logger.info("Navigating to MF dashboard...")
        driver.get(self.MF_URL)
        time.sleep(2)
        try:
            accept_btn = driver.find_element(By.ID, "onetrust-accept-btn-handler")
            if accept_btn.is_displayed():
                accept_btn.click()
                self.logger.info("Accepted cookies.")
                time.sleep(1)
        except Exception:
            self.logger.debug("No cookie accept button found.")

        user = os.environ.get("MF_USER")
        pw = os.environ.get("MF_PASS")
        if not user or not pw:
            raise RuntimeError("MF_USER and MF_PASS environment variables must be set.")
        user_box = driver.find_element(By.ID, "USERID")
        pw_box = driver.find_element(By.ID, "PASSWORD")
        user_box.clear()
        user_box.send_keys(user)
        pw_box.clear()
        pw_box.send_keys(pw)
        login_btn = driver.find_element(By.ID, "logInButton")
        login_btn.click()
        time.sleep(4)

        wait = WebDriverWait(driver, 15)
        code_input = None
        try:
            self.wait_if_human_check()
            try:
                code_input = wait.until(
                    lambda d: d.find_element(By.ID, "TOKEN_VALUE") if d.find_element(By.ID, "TOKEN_VALUE").is_displayed() else None
                )
                self.logger.debug("Found and visible: TOKEN_VALUE")
            except TimeoutException:
                try:
                    code_input = wait.until(
                        lambda d: d.find_element(By.ID, "validationCode") if d.find_element(By.ID, "validationCode").is_displayed() else None
                    )
                    self.logger.debug("Found and visible: validationCode")
                except TimeoutException:
                    self.logger.debug("No visible verification input appeared within 15s.")

            if code_input:
                code = fetch_latest_verification_code(journal="MF")
                self.logger.debug(f"Verification code fetched: {code!r}")
                if code:
                    code_input.clear()
                    code_input.send_keys(code)
                    code_input.send_keys(Keys.ENTER)
                    time.sleep(4)
                    self.activation_required = True
                    self.logger.info("Submitted verification code.")
                else:
                    self.logger.error("No verification code available.")
                    raise RuntimeError("No verification code available.")
        except Exception as e:
            self.logger.error(f"Error waiting for verification prompt or submitting code: {e}")

        with open("mf_postlogin_dashboard.html", "w", encoding="utf-8") as f:
            f.write(driver.page_source)
        self.wait_if_human_check()
        self.logger.info("Ready to continue navigation after login.")

    def parse_referee_list_from_html(self, html, flagged_emails=None, ms_id=None):
        soup = BeautifulSoup(html, "html.parser")
        referee_list = []
        seen_names = set()
        today = datetime.now().date()
        reviewer_tables = []
        for header in soup.find_all("td", class_="detailsheaderbg"):
            if header.find(string=re.compile(r"Reviewer List", re.I)):
                table = header.find_parent("table")
                if table:
                    next_tables = table.find_all_next("table")
                    for nt in next_tables:
                        if nt.find("select", attrs={"name": re.compile(r"ORDER\d*")}):
                            reviewer_tables.append(nt)
                            break
                break

        for reviewer_table in reviewer_tables:
            for tr in reviewer_table.find_all("tr"):
                if tr.find("select", attrs={"name": re.compile(r"ORDER\d*")}):
                    tds = tr.find_all("td", class_="tablelightcolor")
                    if not tds or len(tds) < 2:
                        continue
                    try:
                        name = ""
                        td2 = tds[1]
                        for a in td2.find_all("a"):
                            txt = a.get_text().strip()
                            if txt and "," in txt:
                                name = normalize_name(txt)
                                break
                        if not name or name in seen_names:
                            continue
                        seen_names.add(name)

                        status_raw = ""
                        if len(tds) > 2:
                            status_raw = tds[2].get_text(" ", strip=True)

                        declined_or_returned = False
                        contacted = accepted = due_date = ""
                        for subtable in tr.find_all("table"):
                            for row in subtable.find_all("tr"):
                                cells = row.find_all("td")
                                if len(cells) == 2:
                                    label = cells[0].get_text(strip=True).lower()
                                    value = cells[1].get_text(strip=True)
                                    if "invited" in label or "contacted" in label:
                                        contacted = value
                                    elif "agreed" in label or "accepted" in label:
                                        accepted = value
                                    elif "due date" in label:
                                        due_date = value
                                    elif "review returned" in label or "declined" in label:
                                        declined_or_returned = True

                        if "declined" in (status_raw or "").lower():
                            declined_or_returned = True

                        if declined_or_returned:
                            continue

                        norm_status = mf_status_normalize(status_raw)
                        email_addr = ""
                        crossmatch_date = ""
                        if flagged_emails is not None and ms_id is not None:
                            crossmatch_date, email_addr = robust_match_email_for_referee_mf(
                                name, ms_id, norm_status, flagged_emails
                            )

                        lateness = ""
                        due_date_dt = robust_parse_date(due_date) if due_date else None
                        if due_date_dt and today > due_date_dt:
                            overdue_days = (today - due_date_dt).days
                            lateness = f"{overdue_days} days late"

                        referee_list.append({
                            "Referee Name": name,
                            "Status": norm_status,
                            "Contacted Date": contacted,
                            "Accepted Date": accepted,
                            "Due Date": due_date,
                            "Email": email_addr,
                            "Lateness": lateness,
                        })
                    except Exception as e:
                        self.logger.debug(f"Error parsing referee row: {e}")

        return referee_list

    def parse_manuscript_panel(self, html, flagged_emails=None):
        soup = BeautifulSoup(html, "html.parser")
        ms_id = ""
        title = ""
        contact_author = ""
        submission_date = ""

        b_tag = soup.find("b", string=re.compile(r"MAFI-\d{4}-\d+(\.R\d+)?"))
        if b_tag:
            ms_id = b_tag.get_text(strip=True)
            ms_id_base = re.sub(r"\.R\d+$", "", ms_id)
        else:
            ms_id_base = ""

        found_title = False
        for p in soup.find_all("p", class_="pagecontents"):
            if ms_id and ms_id in p.get_text(strip=True):
                found_title = True
            elif found_title and not title:
                title = p.get_text(strip=True)
                break

        for p in soup.find_all("p", class_="pagecontents"):
            m = re.search(r"([A-Za-z\-]+), ([A-Za-z\-]+)\s*\(contact\)", p.get_text())
            if m:
                contact_author = normalize_name(f"{m.group(1)}, {m.group(2)}")
                break

        for p in soup.find_all("p", class_="footer"):
            m = re.search(r"Submitted:\s*([\d\w\-]+);", p.get_text())
            if m:
                submission_date = m.group(1)
                break

        referees = self.parse_referee_list_from_html(html, flagged_emails=flagged_emails, ms_id=ms_id_base)
        return {
            "Manuscript #": ms_id_base,
            "Title": title,
            "Contact Author": contact_author,
            "Submission Date": submission_date,
            "Referees": referees
        }

    def scrape_manuscripts_and_emails(self):
        driver = self.driver
        self.login()
        time.sleep(2)
        self.wait_if_human_check()
        flagged_emails = fetch_starred_emails("MF")

        found = False
        for attempt in range(12):
            links = driver.find_elements(By.XPATH, "//a")
            for link in links:
                try:
                    link_text = link.text.strip().lower().replace('\xa0', ' ')
                    # Try multiple variations
                    if any(phrase in link_text for phrase in [
                        "associate editor center", 
                        "associate editor centre",
                        "ae center",
                        "ae centre",
                        "editor center",
                        "editor centre"
                    ]):
                        driver.execute_script("arguments[0].scrollIntoView(true);", link)
                        link.click()
                        time.sleep(2)
                        found = True
                        self.logger.debug(f"Clicked Associate Editor Center link: {link_text}")
                        break
                except Exception as e:
                    self.logger.debug(f"Error checking link: {e}")
                    continue
            if found:
                break
            time.sleep(2)
        
        if not found:
            # Try alternative selectors
            self.logger.debug("Trying alternative selectors for Associate Editor Center...")
            try:
                # Try by href
                ae_links = driver.find_elements(By.XPATH, "//a[contains(@href, 'associate') or contains(@href, 'editor')]")
                for link in ae_links:
                    link_text = link.text.strip().lower()
                    if any(word in link_text for word in ["associate", "editor", "ae"]):
                        driver.execute_script("arguments[0].scrollIntoView(true);", link)
                        link.click()
                        time.sleep(2)
                        found = True
                        self.logger.debug(f"Clicked AE link via href: {link_text}")
                        break
            except Exception as e:
                self.logger.debug(f"Error with alternative selectors: {e}")
        
        if not found:
            self.logger.error("Could not find or click Associate Editor Center after login.")
            # Save page source for debugging
            with open("mf_no_ae_center.html", "w", encoding="utf-8") as f:
                f.write(driver.page_source)
            return []

        self.wait_if_human_check()
        dashboard_html = driver.page_source
        soup = BeautifulSoup(dashboard_html, "html.parser")

        desired_statuses = [
            "Awaiting Reviewer Assignment",
            "Awaiting Reviewer Scores",
            "Overdue Reviewer Scores",
        ]

        manuscript_results = {}
        status_found = False

        tds = soup.find_all("td")
        for i in range(len(tds) - 1):
            num_td, status_td = tds[i], tds[i+1]
            status = status_td.get_text(strip=True)
            if status in desired_statuses:
                num_str = num_td.get_text(strip=True)
                if num_str.isdigit() and int(num_str) > 0:
                    status_found = True
                    self.logger.debug(f"Found status '{status}' with count {num_str}")
                    try:
                        link = num_td.find("a")
                        if link and link.get("href"):
                            href = link.get("href")
                            link_elem = driver.find_element(By.XPATH, f"//a[@href='{href}']")
                            driver.execute_script("arguments[0].scrollIntoView(true);", link_elem)
                            link_elem.click()
                        else:
                            cell_elem = driver.find_element(By.XPATH, f"//*[text()='{num_str}']")
                            driver.execute_script("arguments[0].scrollIntoView(true);", cell_elem)
                            cell_elem.click()
                        self.logger.debug(f"Clicked number link for status '{status}' ({num_str})")
                        time.sleep(3)
                        self.wait_if_human_check()

                        ms_html = driver.page_source
                        ms_soup = BeautifulSoup(ms_html, "html.parser")
                        for table in ms_soup.find_all("table"):
                            if table.find("b", string=re.compile(r"MAFI-\d{4}-\d+")):
                                mdata = self.parse_manuscript_panel(str(table), flagged_emails=flagged_emails)
                                ms_id = mdata["Manuscript #"]
                                if ms_id and ms_id not in manuscript_results:
                                    manuscript_results[ms_id] = mdata

                        # Return to AE center after detail view
                        for _ in range(8):
                            try:
                                links = driver.find_elements(By.XPATH, "//a")
                                for link in links:
                                    link_text = link.text.strip().lower().replace('\xa0', ' ')
                                    if "associate editor center" in link_text or "assignment center" in link_text:
                                        driver.execute_script("arguments[0].scrollIntoView(true);", link)
                                        link.click()
                                        time.sleep(2)
                                        break
                            except Exception as e:
                                self.logger.debug(f"Failed to find Associate Editor Center link to return: {e}")
                                time.sleep(1)
                        self.wait_if_human_check()
                        dashboard_html = driver.page_source
                        soup = BeautifulSoup(dashboard_html, "html.parser")
                        tds = soup.find_all("td")
                    except Exception as e:
                        self.logger.debug(f"Failed to click or parse for status '{status}': {e}")

        if not status_found:
            self.logger.debug("No relevant status queues with count > 0 found.")

        self.logger.info(f"Final results (all parsed manuscripts): {len(manuscript_results)} manuscripts.")
        return list(manuscript_results.values())